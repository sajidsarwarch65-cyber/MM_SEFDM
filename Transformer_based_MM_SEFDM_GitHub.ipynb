{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7680ed-bc68-424f-b5e5-189f9c6ca839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Works well for M=4,N=4, groups, have permutation and constellation rotation\n",
    "# MM-SEFDM: Complete Standalone Working Code\n",
    "# All functions included - ready to run independently\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import permutations\n",
    "from math import factorial, log2, floor\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ============ Helper Functions ============\n",
    "\n",
    "def simple_progress(iterable, desc=\"Progress\", leave=True):\n",
    "    \"\"\"Simple progress indicator\"\"\"\n",
    "    total = len(iterable) if hasattr(iterable, '__len__') else None\n",
    "    for i, item in enumerate(iterable):\n",
    "        if total and ((i + 1) % max(1, total // 10) == 0 or (i + 1) == total):\n",
    "            print(f\"\\r{desc}: {i+1}/{total}\", end='', flush=True)\n",
    "        yield item\n",
    "    if leave:\n",
    "        print()\n",
    "\n",
    "# ============ Signal Processing (From Working Code) ============\n",
    "\n",
    "def sefdm_modulate(freq_signal, alpha):\n",
    "    \"\"\"SEFDM modulation with bandwidth compression alpha\"\"\"\n",
    "    N = len(freq_signal)\n",
    "    time_signal = np.zeros(N, dtype=complex)\n",
    "    for n in range(N):\n",
    "        for k in range(N):\n",
    "            time_signal[n] += freq_signal[k] * np.exp(2j * np.pi * k * alpha * n / N)\n",
    "    time_signal /= np.sqrt(N)\n",
    "    return time_signal\n",
    "\n",
    "def generate_multimode_lut(N, M):\n",
    "    \"\"\"Generate multi-mode LUT with mode-dependent constellations\"\"\"\n",
    "    all_perms = list(permutations(range(N)))\n",
    "    index_bits = floor(log2(factorial(N)))\n",
    "    num_valid_perms = 2 ** index_bits\n",
    "    valid_perms = all_perms[:num_valid_perms]\n",
    "\n",
    "    base_symbols = np.exp(2j * np.pi * np.arange(M) / M)\n",
    "    mode_constants = [np.exp(2j * np.pi * q / (N * M)) for q in range(N)]\n",
    "    mode_constellations = [mode_constants[k] * base_symbols for k in range(N)]\n",
    "\n",
    "    lut = {}\n",
    "    label = 0\n",
    "\n",
    "    for perm in valid_perms:\n",
    "        for symbol_indices in np.ndindex(*([M]*N)):\n",
    "            freq_signal = np.zeros(N, dtype=complex)\n",
    "            for pos_idx, subcarrier_idx in enumerate(perm):\n",
    "                symbol = mode_constellations[pos_idx][symbol_indices[pos_idx]]\n",
    "                freq_signal[subcarrier_idx] = symbol\n",
    "            lut[label] = {'freq_signal': freq_signal}\n",
    "            label += 1\n",
    "\n",
    "    bits_per_symbol = int(np.log2(len(lut)))\n",
    "    print(f\"LUT: {len(lut)} entries, {bits_per_symbol} bits/group\")\n",
    "    return lut, bits_per_symbol\n",
    "\n",
    "def generate_rayleigh_channel(N, channel_model='flat'):\n",
    "    \"\"\"Generate Rayleigh fading channel\"\"\"\n",
    "    if channel_model == 'flat':\n",
    "        h = (np.random.randn() + 1j * np.random.randn()) / np.sqrt(2)\n",
    "        return h * np.ones(N, dtype=complex)\n",
    "    return (np.random.randn(N) + 1j * np.random.randn(N)) / np.sqrt(2)\n",
    "\n",
    "def add_awgn(signal, snr_db):\n",
    "    \"\"\"Add AWGN noise to signal\"\"\"\n",
    "    if snr_db == np.inf:\n",
    "        return signal\n",
    "    snr_linear = 10 ** (snr_db / 10)\n",
    "    signal_power = np.mean(np.abs(signal)**2)\n",
    "    noise_power = signal_power / snr_linear\n",
    "    return signal + np.sqrt(noise_power / 2) * (np.random.randn(*signal.shape) +\n",
    "                                                  1j * np.random.randn(*signal.shape))\n",
    "\n",
    "# ============ Custom Layers ============\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    \"\"\"Trainable positional encoding\"\"\"\n",
    "    def __init__(self, d_model, max_len=10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.pos_embedding = self.add_weight(\n",
    "            name='pos_embedding',\n",
    "            shape=(self.max_len, self.d_model),\n",
    "            initializer='uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        positions = self.pos_embedding[:seq_len, :]\n",
    "        positions = tf.expand_dims(positions, 0)\n",
    "        positions = tf.tile(positions, [batch_size, 1, 1])\n",
    "        return x + positions\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"d_model\": self.d_model, \"max_len\": self.max_len})\n",
    "        return config\n",
    "\n",
    "class ChannelAdaptiveGating(layers.Layer):\n",
    "    \"\"\"Channel-adaptive gating mechanism\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = input_shape[0][-1]\n",
    "        self.gate_net = keras.Sequential([\n",
    "            layers.Dense(feature_dim * 2, activation='relu'),\n",
    "            layers.Dense(feature_dim, activation='sigmoid')\n",
    "        ], name='gate_network')\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        features, channel_conditions = inputs\n",
    "        gates = self.gate_net(channel_conditions)\n",
    "        if len(features.shape) == 3:\n",
    "            gates = tf.expand_dims(gates, 1)\n",
    "        return features * gates\n",
    "\n",
    "    def get_config(self):\n",
    "        return super().get_config()\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\"Transformer block with multi-head attention\"\"\"\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.att = layers.MultiHeadAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            key_dim=self.d_model // self.num_heads,\n",
    "            dropout=self.dropout_rate\n",
    "        )\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(self.ff_dim, activation='gelu'),\n",
    "            layers.Dropout(self.dropout_rate),\n",
    "            layers.Dense(self.d_model),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(self.dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(self.dropout_rate)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        attn_output = self.att(x, x, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"dropout\": self.dropout_rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# ============ Architecture ============\n",
    "\n",
    "def build_advanced_cnn_transformer_hybrid(input_shape, num_classes,\n",
    "                                         d_model=64, num_heads=4, num_layers=2,\n",
    "                                         ff_dim=128, dropout=0.15):\n",
    "    \"\"\"\n",
    "    Complete working architecture:\n",
    "    - CNN with residual connections\n",
    "    - Trainable positional encoding\n",
    "    - Channel-adaptive gating\n",
    "    - Transformer blocks with multi-head attention\n",
    "    - Dual pooling\n",
    "    - GELU activations\n",
    "    \"\"\"\n",
    "    signal_input = layers.Input(shape=input_shape, name='signal_input')\n",
    "    alpha_input = layers.Input(shape=(1,), name='alpha_input')\n",
    "    snr_input = layers.Input(shape=(1,), name='snr_input')\n",
    "    channel_type_input = layers.Input(shape=(1,), name='channel_type_input')\n",
    "\n",
    "    # CNN with residual connections\n",
    "    conv1 = layers.Conv1D(32, 3, padding='same', activation='relu', name='conv1_k3')(signal_input)\n",
    "    conv1 = layers.LayerNormalization(name='conv1_ln')(conv1)\n",
    "    conv1 = layers.Dropout(dropout, name='conv1_dropout')(conv1)\n",
    "\n",
    "    conv2 = layers.Conv1D(48, 5, padding='same', activation='relu', name='conv2_k5')(conv1)\n",
    "    conv2 = layers.LayerNormalization(name='conv2_ln')(conv2)\n",
    "    conv2 = layers.Dropout(dropout, name='conv2_dropout')(conv2)\n",
    "\n",
    "    conv3 = layers.Conv1D(64, 7, padding='same', activation='relu', name='conv3_k7')(conv2)\n",
    "    conv3 = layers.LayerNormalization(name='conv3_ln')(conv3)\n",
    "\n",
    "    residual = layers.Conv1D(64, 1, name='residual_proj')(signal_input)\n",
    "    cnn_features = layers.Add(name='residual_add')([conv3, residual])\n",
    "    cnn_features = layers.Dropout(dropout, name='conv_final_dropout')(cnn_features)\n",
    "\n",
    "    # Channel embeddings (alpha, SNR, channel type)\n",
    "    alpha_embed = layers.Dense(64, activation='relu')(alpha_input)\n",
    "    alpha_embed = layers.LayerNormalization()(alpha_embed)\n",
    "    alpha_embed = layers.Dense(64, activation='relu')(alpha_embed)\n",
    "    alpha_embed = layers.LayerNormalization()(alpha_embed)\n",
    "    alpha_embed = layers.Dense(d_model, activation='tanh')(alpha_embed)\n",
    "\n",
    "    snr_embed = layers.Dense(64, activation='relu')(snr_input)\n",
    "    snr_embed = layers.LayerNormalization()(snr_embed)\n",
    "    snr_embed = layers.Dense(d_model, activation='tanh')(snr_embed)\n",
    "\n",
    "    channel_type_embed = layers.Dense(32, activation='relu')(channel_type_input)\n",
    "    channel_type_embed = layers.Dense(d_model//2, activation='tanh')(channel_type_embed)\n",
    "\n",
    "    channel_conditions = layers.Concatenate()([alpha_embed, snr_embed, channel_type_embed])\n",
    "    channel_conditions = layers.Dense(d_model*2, activation='gelu')(channel_conditions)\n",
    "    channel_conditions = layers.Dropout(dropout)(channel_conditions)\n",
    "    channel_conditions = layers.Dense(d_model, activation='tanh')(channel_conditions)\n",
    "\n",
    "    # Channel-adaptive gating\n",
    "    cnn_features_gated = ChannelAdaptiveGating()([cnn_features, channel_conditions])\n",
    "\n",
    "    # Positional encoding + Transformer\n",
    "    x = PositionalEncoding(d_model, max_len=input_shape[0])(cnn_features_gated)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        x = TransformerBlock(d_model, num_heads, ff_dim, dropout)(x)\n",
    "\n",
    "    # Dual pooling\n",
    "    global_avg = layers.GlobalAveragePooling1D()(x)\n",
    "    global_max = layers.GlobalMaxPooling1D()(x)\n",
    "    pooled = layers.Concatenate()([global_avg, global_max])\n",
    "\n",
    "    # Fusion with channel conditions\n",
    "    combined = layers.Concatenate()([pooled, channel_conditions])\n",
    "\n",
    "    # Classification head\n",
    "    x = layers.Dense(256, activation='gelu')(combined)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Dropout(dropout*1.5)(x)\n",
    "\n",
    "    x = layers.Dense(128, activation='gelu')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[signal_input, alpha_input, snr_input, channel_type_input],\n",
    "        outputs=outputs,\n",
    "        name='Advanced_CNN_Transformer_Hybrid'\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# ============ Data Generation ============\n",
    "\n",
    "def generate_curriculum_data(lut, group_size, alpha_range, snr_range, num_samples,\n",
    "                            channel_type='awgn', curriculum_level='easy'):\n",
    "    \"\"\"Generate training data with curriculum strategy\"\"\"\n",
    "    print(f\"\\nGenerating {num_samples} samples (Curriculum: {curriculum_level})\")\n",
    "    print(f\"  Channel: {channel_type}\")\n",
    "    print(f\"  Alpha: {alpha_range[0]}-{alpha_range[1]}\")\n",
    "    print(f\"  SNR: {snr_range[0]}-{snr_range[1]} dB\")\n",
    "\n",
    "    alpha_low, alpha_high = alpha_range\n",
    "    snr_low, snr_high = snr_range\n",
    "\n",
    "    if curriculum_level == 'easy':\n",
    "        alpha_bins = np.linspace(alpha_low, alpha_high, 5)\n",
    "        snr_bins = np.linspace(snr_low, snr_high, 6)\n",
    "    else:\n",
    "        alpha_bins = np.linspace(alpha_low, alpha_high, 7)\n",
    "        snr_bins = np.linspace(snr_low, snr_high, 8)\n",
    "\n",
    "    samples_per_stratum = max(1, num_samples // (len(alpha_bins) * len(snr_bins)))\n",
    "\n",
    "    X_real, X_imag, y = [], [], []\n",
    "    alpha_values, snr_values, channel_types = [], [], []\n",
    "\n",
    "    total_samples = len(alpha_bins) * len(snr_bins) * samples_per_stratum\n",
    "    sample_count = 0\n",
    "\n",
    "    for i in range(len(alpha_bins)):\n",
    "        for j in range(len(snr_bins)):\n",
    "            for _ in range(samples_per_stratum):\n",
    "                sample_count += 1\n",
    "                if sample_count % (total_samples // 10) == 0:\n",
    "                    print(f\"Progress: {sample_count}/{total_samples}\", end='\\r', flush=True)\n",
    "\n",
    "                label = np.random.randint(0, len(lut))\n",
    "\n",
    "                alpha = np.random.uniform(alpha_bins[i],\n",
    "                                         alpha_bins[min(i+1, len(alpha_bins)-1)])\n",
    "                snr_db = np.random.uniform(snr_bins[j],\n",
    "                                          snr_bins[min(j+1, len(snr_bins)-1)])\n",
    "\n",
    "                tx_freq = lut[label]['freq_signal']\n",
    "                tx_time = sefdm_modulate(tx_freq, alpha)\n",
    "\n",
    "                if channel_type == 'awgn':\n",
    "                    channel = np.ones(group_size, dtype=complex)\n",
    "                    channel_type_val = 0.0\n",
    "                else:\n",
    "                    channel = generate_rayleigh_channel(group_size, 'flat')\n",
    "                    channel_type_val = 1.0\n",
    "\n",
    "                rx_time = tx_time * channel\n",
    "                rx_time = add_awgn(rx_time, snr_db)\n",
    "                rx_time_eq = rx_time / (channel + 1e-10)\n",
    "\n",
    "                X_real.append(np.real(rx_time_eq))\n",
    "                X_imag.append(np.imag(rx_time_eq))\n",
    "                y.append(label)\n",
    "                alpha_values.append(alpha)\n",
    "                snr_values.append(snr_db)\n",
    "                channel_types.append(channel_type_val)\n",
    "\n",
    "    print()\n",
    "\n",
    "    X = np.stack([np.array(X_real), np.array(X_imag)], axis=-1).astype(np.float32)\n",
    "    y = np.array(y, dtype=np.int32)\n",
    "    alpha_values = np.array(alpha_values, dtype=np.float32).reshape(-1, 1)\n",
    "    snr_values = np.array(snr_values, dtype=np.float32).reshape(-1, 1)\n",
    "    channel_types = np.array(channel_types, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "    X_mean, X_std = np.mean(X), np.std(X)\n",
    "    X = (X - X_mean) / (X_std + 1e-8)\n",
    "\n",
    "    alpha_normalized = (alpha_values - alpha_low) / (alpha_high - alpha_low)\n",
    "    snr_normalized = (snr_values - snr_low) / (snr_high - snr_low)\n",
    "\n",
    "    print(f\"Data: X={X.shape}\")\n",
    "    return (X, alpha_normalized, snr_normalized, channel_types, y,\n",
    "            X_mean, X_std, alpha_low, alpha_high, snr_low, snr_high)\n",
    "\n",
    "# ============ Testing Functions ============\n",
    "\n",
    "def transmit_receive_grouped_system(labels, lut, N_total, groups, alpha, snr_db,\n",
    "                                    channel_type='rayleigh', rayleigh_model='flat'):\n",
    "    \"\"\"Transmit and receive for grouped system\"\"\"\n",
    "    group_size = N_total // groups\n",
    "    tx_time_total = np.zeros(N_total, dtype=complex)\n",
    "\n",
    "    for g, label in enumerate(labels):\n",
    "        tx_freq = lut[label]['freq_signal']\n",
    "        tx_time = sefdm_modulate(tx_freq, alpha)\n",
    "        tx_time_total[g*group_size:(g+1)*group_size] = tx_time\n",
    "\n",
    "    if channel_type == 'awgn':\n",
    "        channel_total = np.ones(N_total, dtype=complex)\n",
    "    else:\n",
    "        channel_total = np.zeros(N_total, dtype=complex)\n",
    "        for g in range(groups):\n",
    "            h = generate_rayleigh_channel(group_size, rayleigh_model)\n",
    "            channel_total[g*group_size:(g+1)*group_size] = h\n",
    "\n",
    "    rx_time_total = tx_time_total * channel_total\n",
    "    rx_time_total = add_awgn(rx_time_total, snr_db)\n",
    "\n",
    "    rx_groups = []\n",
    "    channel_groups = []\n",
    "    for g in range(groups):\n",
    "        rx_groups.append(rx_time_total[g*group_size:(g+1)*group_size])\n",
    "        channel_groups.append(channel_total[g*group_size:(g+1)*group_size])\n",
    "\n",
    "    return rx_groups, channel_groups\n",
    "\n",
    "def test_ber(model, lut, N_total, groups, alpha_test, snr_test,\n",
    "            X_mean, X_std, alpha_train_range, snr_train_range,\n",
    "            num_symbols=1000, channel_type='rayleigh'):\n",
    "    \"\"\"Test BER\"\"\"\n",
    "    group_size = N_total // groups\n",
    "    bits_per_group = int(np.log2(len(lut)))\n",
    "\n",
    "    alpha_low, alpha_high = alpha_train_range\n",
    "    snr_low, snr_high = snr_train_range\n",
    "\n",
    "    total_bit_errors = 0\n",
    "    total_bits = 0\n",
    "\n",
    "    channel_type_value = 0.0 if channel_type == 'awgn' else 1.0\n",
    "\n",
    "    for i in simple_progress(range(num_symbols),\n",
    "                            desc=f\"  α={alpha_test:.2f}, SNR={snr_test}dB\",\n",
    "                            leave=False):\n",
    "        true_labels = [np.random.randint(0, len(lut)) for _ in range(groups)]\n",
    "\n",
    "        rx_groups, channel_groups = transmit_receive_grouped_system(\n",
    "            true_labels, lut, N_total, groups, alpha_test, snr_test,\n",
    "            channel_type, 'flat'\n",
    "        )\n",
    "\n",
    "        for g in range(groups):\n",
    "            rx_eq = rx_groups[g] / (channel_groups[g] + 1e-10)\n",
    "            X_test = np.stack([np.real(rx_eq), np.imag(rx_eq)], axis=-1)\n",
    "            X_test = X_test.reshape(1, group_size, 2)\n",
    "            X_test = (X_test - X_mean) / (X_std + 1e-8)\n",
    "\n",
    "            alpha_norm = np.array([[(alpha_test - alpha_low) / (alpha_high - alpha_low)]])\n",
    "            snr_norm = np.array([[(snr_test - snr_low) / (snr_high - snr_low)]])\n",
    "            channel_type_norm = np.array([[channel_type_value]])\n",
    "\n",
    "            pred = model.predict([X_test, alpha_norm, snr_norm, channel_type_norm], verbose=0)\n",
    "            detected_label = np.argmax(pred[0])\n",
    "\n",
    "            true_bits = np.array([int(b) for b in format(true_labels[g], f'0{bits_per_group}b')])\n",
    "            det_bits = np.array([int(b) for b in format(detected_label, f'0{bits_per_group}b')])\n",
    "            total_bit_errors += np.sum(true_bits != det_bits)\n",
    "            total_bits += bits_per_group\n",
    "\n",
    "    ber = total_bit_errors / total_bits if total_bits > 0 else 0\n",
    "    return max(ber, 1e-7)\n",
    "\n",
    "# ============ Main Training Script ============\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # System parameters\n",
    "    N_TOTAL = 16\n",
    "    GROUPS = 4\n",
    "    N = N_TOTAL // GROUPS  # N=4\n",
    "    M = 2\n",
    "\n",
    "    # Curriculum parameters\n",
    "    BASE_ALPHA_RANGE = (0.5, 1.0)\n",
    "    BASE_SNR_RANGE = (0, 35)\n",
    "\n",
    "    PHASE1_ALPHA_RANGE = (0.7, 1.0)\n",
    "    PHASE1_SNR_RANGE = (15, 35)\n",
    "\n",
    "    # Samples\n",
    "    NUM_TRAIN_P1 = 200000\n",
    "    NUM_VAL_P1 = 40000\n",
    "    NUM_TRAIN_P2 = 200000\n",
    "    NUM_VAL_P2 = 40000\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCHS_P1 = 10\n",
    "    EPOCHS_P2 = 60\n",
    "\n",
    "    # Test parameters\n",
    "    ALPHA_TEST_VALUES = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    SNR_TEST_VALUES = np.arange(0, 36, 5)\n",
    "    NUM_TEST = 1000\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"MM-SEFDM: COMPLETE STANDALONE CODE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"System: N={N}, Groups={GROUPS}, M={M}-PSK\")\n",
    "    print(f\"Architecture: CNN+PosEnc+ChannelGating+Transformer\")\n",
    "    print(f\"\\nPhase 1 (AWGN, Easy): α∈{PHASE1_ALPHA_RANGE}, SNR∈{PHASE1_SNR_RANGE}\")\n",
    "    print(f\"Phase 2 (Rayleigh, Full): α∈{BASE_ALPHA_RANGE}, SNR∈{BASE_SNR_RANGE}\")\n",
    "    print(f\"Strategy: Fresh model for each phase\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Generate LUT\n",
    "    lut, bits_per_group = generate_multimode_lut(N, M)\n",
    "    num_classes = len(lut)\n",
    "\n",
    "    save_path = f\"./MM_SEFDM_Standalone_N{N}\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    weights_file_p1 = os.path.join(save_path, \"phase1_weights.h5\")\n",
    "    weights_file_p2 = os.path.join(save_path, \"phase2_weights.h5\")\n",
    "\n",
    "    # ============ PHASE 1: AWGN Pretraining ============\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PHASE 1: AWGN PRETRAINING (Easy Curriculum)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    X_train_p1, alpha_train_p1, snr_train_p1, channel_type_train_p1, y_train_p1, \\\n",
    "    X_mean_p1, X_std_p1, alpha_low_p1, alpha_high_p1, snr_low_p1, snr_high_p1 = \\\n",
    "        generate_curriculum_data(\n",
    "            lut, N, PHASE1_ALPHA_RANGE, PHASE1_SNR_RANGE, NUM_TRAIN_P1,\n",
    "            channel_type='awgn', curriculum_level='easy'\n",
    "        )\n",
    "\n",
    "    X_val_p1, alpha_val_p1, snr_val_p1, channel_type_val_p1, y_val_p1, _, _, _, _, _, _ = \\\n",
    "        generate_curriculum_data(\n",
    "            lut, N, PHASE1_ALPHA_RANGE, PHASE1_SNR_RANGE, NUM_VAL_P1,\n",
    "            channel_type='awgn', curriculum_level='easy'\n",
    "        )\n",
    "\n",
    "    print(\"\\n>>> Building Phase 1 model...\")\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    model_p1 = build_advanced_cnn_transformer_hybrid(\n",
    "        (N, 2), num_classes, d_model=64, num_heads=4, num_layers=2,\n",
    "        ff_dim=128, dropout=0.15\n",
    "    )\n",
    "\n",
    "    model_p1.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    print(f\"Parameters: {model_p1.count_params():,}\")\n",
    "\n",
    "    callbacks_p1 = [\n",
    "        EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True,\n",
    "                     mode='max', verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3,\n",
    "                        min_lr=1e-6, verbose=1),\n",
    "        ModelCheckpoint(weights_file_p1, monitor='val_accuracy', save_best_only=True,\n",
    "                       mode='max', save_weights_only=True, verbose=1)\n",
    "    ]\n",
    "\n",
    "    print(\"\\n>>> Training Phase 1...\")\n",
    "    history_p1 = model_p1.fit(\n",
    "        [X_train_p1, alpha_train_p1, snr_train_p1, channel_type_train_p1], y_train_p1,\n",
    "        validation_data=([X_val_p1, alpha_val_p1, snr_val_p1, channel_type_val_p1], y_val_p1),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS_P1,\n",
    "        callbacks=callbacks_p1,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    best_acc_p1 = max(history_p1.history['val_accuracy'])\n",
    "    print(f\"\\n✓ Phase 1 Complete! Best val accuracy: {best_acc_p1:.4f}\")\n",
    "\n",
    "    del X_train_p1, alpha_train_p1, snr_train_p1, channel_type_train_p1, y_train_p1\n",
    "    del X_val_p1, alpha_val_p1, snr_val_p1, channel_type_val_p1, y_val_p1\n",
    "    del model_p1\n",
    "    gc.collect()\n",
    "\n",
    "    # ============ PHASE 2: Rayleigh Training ============\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PHASE 2: RAYLEIGH TRAINING (Full Curriculum, FRESH MODEL)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    X_train_p2, alpha_train_p2, snr_train_p2, channel_type_train_p2, y_train_p2, \\\n",
    "    X_mean_p2, X_std_p2, alpha_low_p2, alpha_high_p2, snr_low_p2, snr_high_p2 = \\\n",
    "        generate_curriculum_data(\n",
    "            lut, N, BASE_ALPHA_RANGE, BASE_SNR_RANGE, NUM_TRAIN_P2,\n",
    "            channel_type='rayleigh', curriculum_level='full'\n",
    "        )\n",
    "\n",
    "    X_val_p2, alpha_val_p2, snr_val_p2, channel_type_val_p2, y_val_p2, _, _, _, _, _, _ = \\\n",
    "        generate_curriculum_data(\n",
    "            lut, N, BASE_ALPHA_RANGE, BASE_SNR_RANGE, NUM_VAL_P2,\n",
    "            channel_type='rayleigh', curriculum_level='full'\n",
    "        )\n",
    "\n",
    "    print(\"\\n>>> Building Phase 2 model (fresh)...\")\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    model_p2 = build_advanced_cnn_transformer_hybrid(\n",
    "        (N, 2), num_classes, d_model=64, num_heads=4, num_layers=2,\n",
    "        ff_dim=128, dropout=0.15\n",
    "    )\n",
    "\n",
    "    model_p2.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4, clipnorm=1.0),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    print(f\"Parameters: {model_p2.count_params():,}\")\n",
    "\n",
    "    callbacks_p2 = [\n",
    "        EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True,\n",
    "                     mode='max', verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3,\n",
    "                        min_lr=1e-6, verbose=1),\n",
    "        ModelCheckpoint(weights_file_p2, monitor='val_accuracy', save_best_only=True,\n",
    "                       mode='max', save_weights_only=True, verbose=1)\n",
    "    ]\n",
    "\n",
    "    print(\"\\n>>> Training Phase 2...\")\n",
    "    history_p2 = model_p2.fit(\n",
    "        [X_train_p2, alpha_train_p2, snr_train_p2, channel_type_train_p2], y_train_p2,\n",
    "        validation_data=([X_val_p2, alpha_val_p2, snr_val_p2, channel_type_val_p2], y_val_p2),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS_P2,\n",
    "        callbacks=callbacks_p2,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    best_acc_p2 = max(history_p2.history['val_accuracy'])\n",
    "    print(f\"\\n✓ Phase 2 Complete! Best val accuracy: {best_acc_p2:.4f}\")\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Phase 1 (AWGN):     {best_acc_p1:.4f}\")\n",
    "    print(f\"Phase 2 (Rayleigh): {best_acc_p2:.4f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    model_p2.load_weights(weights_file_p2)\n",
    "\n",
    "    with open(os.path.join(save_path, 'stats.pkl'), 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'X_mean': X_mean_p2, 'X_std': X_std_p2,\n",
    "            'alpha_low': alpha_low_p2, 'alpha_high': alpha_high_p2,\n",
    "            'snr_low': snr_low_p2, 'snr_high': snr_high_p2\n",
    "        }, f)\n",
    "\n",
    "    del X_train_p2, alpha_train_p2, snr_train_p2, channel_type_train_p2, y_train_p2\n",
    "    del X_val_p2, alpha_val_p2, snr_val_p2, channel_type_val_p2, y_val_p2\n",
    "    gc.collect()\n",
    "\n",
    "    # ============ TESTING ============\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TESTING ON RAYLEIGH CHANNEL\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    ber_grid = np.zeros((len(ALPHA_TEST_VALUES), len(SNR_TEST_VALUES)))\n",
    "\n",
    "    for i, alpha_test in enumerate(ALPHA_TEST_VALUES):\n",
    "        print(f\"\\nTesting Alpha = {alpha_test:.2f}\")\n",
    "        for j, snr_test in enumerate(SNR_TEST_VALUES):\n",
    "            ber = test_ber(\n",
    "                model_p2, lut, N_TOTAL, GROUPS, alpha_test, snr_test,\n",
    "                X_mean_p2, X_std_p2,\n",
    "                (alpha_low_p2, alpha_high_p2),\n",
    "                (snr_low_p2, snr_high_p2),\n",
    "                num_symbols=NUM_TEST,\n",
    "                channel_type='rayleigh'\n",
    "            )\n",
    "            ber_grid[i, j] = ber\n",
    "            print(f\"  SNR={snr_test:2d} dB: BER={ber:.6e}\")\n",
    "\n",
    "    with open(os.path.join(save_path, 'results.pkl'), 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'alpha_values': ALPHA_TEST_VALUES,\n",
    "            'snr_values': list(SNR_TEST_VALUES),\n",
    "            'ber_grid': ber_grid.tolist(),\n",
    "            'best_acc_p1': best_acc_p1,\n",
    "            'best_acc_p2': best_acc_p2\n",
    "        }, f)\n",
    "\n",
    "    # ============ PLOTTING ============\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(ALPHA_TEST_VALUES)))\n",
    "\n",
    "    for i, alpha_val in enumerate(ALPHA_TEST_VALUES):\n",
    "        label = f'α={alpha_val}' + (' (OFDM)' if alpha_val == 1.0 else '')\n",
    "        plt.semilogy(SNR_TEST_VALUES, ber_grid[i, :],\n",
    "                    marker='o', linewidth=2.5, markersize=7,\n",
    "                    color=colors[i], label=label)\n",
    "\n",
    "    plt.xlabel('SNR (dB)', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('BER', fontsize=14, fontweight='bold')\n",
    "    plt.title(f'MM-SEFDM: Complete Architecture\\n'\n",
    "              f'CNN+PosEnc+ChannelGating+Transformer\\n'\n",
    "              f'N={N}, {num_classes} classes, Groups={GROUPS}',\n",
    "              fontsize=12, fontweight='bold')\n",
    "    plt.grid(True, which='both', alpha=0.3)\n",
    "    plt.legend(loc='best', fontsize=11)\n",
    "    plt.ylim([1e-5, 1])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, 'ber_standalone.png'), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"✓ TRAINING AND TESTING COMPLETE!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Results saved to: {save_path}\")\n",
    "    print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MM_SEFDM_trans)",
   "language": "python",
   "name": "mm_sefdm_trans"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
